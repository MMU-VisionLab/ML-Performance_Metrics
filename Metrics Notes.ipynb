{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix, Accuracy, Precision, Recall and F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A summary table that shows the predictions of our model on the given examples.\n",
    "* Used for classification problems\n",
    "* Table with two dimensions\n",
    "    * Actual\n",
    "    * Predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "![](CM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Source : https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **True Positives**  : When the data's actual class and the predicted class are both **True**.\n",
    "2. **True Negatives**  : When the data's actual class and the predicted class are both **False**.\n",
    "3. **False Positives** : When the data's actual class is **False** but the predicted class is **True**.\n",
    "4. **False Negatives** : When the data's actual class is **True** but the predicted class is **False**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimizing errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the use case, we either minimize **False Positives** or **False Negatives** or **both**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E.g.** In a problem where we predict whether a patient has a certain disease (**Positive**) or not (**Negative**), we want to minimize **FN** since falsely predicting that a person has no cancer is far more dangerous than falsely predicting that the person has cancer since the person will go through even more analysis anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E.g.** In a problem where we want to classify whether an email is Spam (**Positive**) or not (**Negative**), we would want to minimize **FP** since falsely predicting an email is Spam could lead to bigger problems than falsely predicting an email is **not** spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is defined as the number of correct predictions made by the model over all the other predictions made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Source : https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The most ideal situation to use this metric is when the data used is split equally (or almost equally) between their actual classes.\n",
    "* For a terrorist identification problem, if the dataset used only have 2% terrorist class data, and if the model just predicts every data as non-terrorist, it'd still have a solid **98% accuracy !** \n",
    "* Accuracy can be used to represent the performance of a model when all the classes are of equal importance. i.e. Both FN and FP are of equal importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is defined as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Precision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Source : https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, precision tells us how often our model correctly predicts a certain class. In a problem where we categorise patients with having cancers or not, our model will have **100% precision** even if the model managed to capture **one** patient with having cancer correctly while categorising the rest of the patients as **not** having cancers. This is due to the rest of the patients that are incorrectly categorised as not having cancers will either fall in **TN** or **FN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is the ratio of correct predictions on a certain class to the total examples that belongs to the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Source : https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a high recall occurs when the **FN** is small, it indicates that the higher the recall the better the class is recognized by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall and Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High Recall, Low Precision**: This means that most of the positive examples are correctly recognized (low FN) but there are a lot of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low Recall, High Precision**: This shows that we miss a lot of positive examples (high FN) but those we predict as positive are indeed positive (low FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score / F-Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is the weighted average of Precision and Recall. It uses Harmonic Mean since it punishes the extreme values more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](f1_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Source : https://www.geeksforgeeks.org/confusion-matrix-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the F1 score, the lower the **FN** and **FP**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these metrics have their own advantages and disadvantages. In practice, there is no one way to follow. Which metrics to use to represent the model's performance depends solely on the business use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa",
   "language": "python",
   "name": "nasa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
